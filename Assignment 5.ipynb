{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31a612c8",
   "metadata": {},
   "source": [
    "# Assignment 5: PCA and Neural Networks (50 marks)\n",
    "### Due: April 4 at 11:59pm \n",
    "\n",
    "### Name: Elias Poitras-Whitecalf\n",
    "### UCID: 30193066\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cb68ecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652ce757",
   "metadata": {},
   "source": [
    "## Part 1: Principle Component Analysis (PCA) (20 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9740443e",
   "metadata": {},
   "source": [
    "### Step 1. Load data (2 marks)\n",
    "\n",
    "You have been asked by an agricultural company to help them predict the type of wheat based on kernel characteristics. You have been given all the information you need in the seeds_dataset.txt file on D2L. The original dataset can be found [here](https://archive.ics.uci.edu/dataset/236/seeds)\n",
    "\n",
    "The first step is to read in the file using pandas and inspect the first few columns. Note that the data is in a text file, so the separator is different compared to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b23df533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area</th>\n",
       "      <th>perimeter</th>\n",
       "      <th>compactness</th>\n",
       "      <th>length of kernel</th>\n",
       "      <th>width of kernel</th>\n",
       "      <th>asymmetry coefficient</th>\n",
       "      <th>length of kernel groove</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.26</td>\n",
       "      <td>14.84</td>\n",
       "      <td>0.8710</td>\n",
       "      <td>5.763</td>\n",
       "      <td>3.312</td>\n",
       "      <td>2.221</td>\n",
       "      <td>5.220</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.88</td>\n",
       "      <td>14.57</td>\n",
       "      <td>0.8811</td>\n",
       "      <td>5.554</td>\n",
       "      <td>3.333</td>\n",
       "      <td>1.018</td>\n",
       "      <td>4.956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.29</td>\n",
       "      <td>14.09</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>5.291</td>\n",
       "      <td>3.337</td>\n",
       "      <td>2.699</td>\n",
       "      <td>4.825</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.84</td>\n",
       "      <td>13.94</td>\n",
       "      <td>0.8955</td>\n",
       "      <td>5.324</td>\n",
       "      <td>3.379</td>\n",
       "      <td>2.259</td>\n",
       "      <td>4.805</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.14</td>\n",
       "      <td>14.99</td>\n",
       "      <td>0.9034</td>\n",
       "      <td>5.658</td>\n",
       "      <td>3.562</td>\n",
       "      <td>1.355</td>\n",
       "      <td>5.175</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    area  perimeter  compactness  length of kernel  width of kernel  \\\n",
       "0  15.26      14.84       0.8710             5.763            3.312   \n",
       "1  14.88      14.57       0.8811             5.554            3.333   \n",
       "2  14.29      14.09       0.9050             5.291            3.337   \n",
       "3  13.84      13.94       0.8955             5.324            3.379   \n",
       "4  16.14      14.99       0.9034             5.658            3.562   \n",
       "\n",
       "   asymmetry coefficient  length of kernel groove  class  \n",
       "0                  2.221                    5.220      1  \n",
       "1                  1.018                    4.956      1  \n",
       "2                  2.699                    4.825      1  \n",
       "3                  2.259                    4.805      1  \n",
       "4                  1.355                    5.175      1  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: Import dataset and inspect the first few columns (1 mark)\n",
    "df = pd.read_csv(\"seeds_dataset.txt\", delimiter=\"\\t\", names=[\"area\", \"perimeter\", \"compactness\", \"length of kernel\",\"width of kernel\",\"asymmetry coefficient\",\"length of kernel groove\",\"class\"]) \n",
    "df.head()\n",
    "#class field 1 is kama, 2 is Rosa, 3 is Canadian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e47059",
   "metadata": {},
   "source": [
    "Next, you will need to separate the data into your feature matrix and target vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ec3f7bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (210, 7)\n",
      "Shape of y: (210,)\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Separate the data into feature matrix and target vector. Print the shape of X and y (1 mark)\n",
    "X = df.iloc[:, :-1]\n",
    "y = df[\"class\"]\n",
    "\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4781acec",
   "metadata": {},
   "source": [
    "### Steps 2+3: Preprocessing and Model Selection (3 marks)\n",
    "\n",
    "The first step is to split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8b3930b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Split the data into training and testing sets using 10% for test set (1 mark)\n",
    "import sklearn.model_selection as sk\n",
    "Xtrain,Xtest,ytrain,ytest = sk.train_test_split(X, y, test_size=0.10, random_state=0) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4d4b84",
   "metadata": {},
   "source": [
    "Now that we have our training data, we need to decide which preprocessing methods to use. Since we do not want any information leaking into the model validation stage, we will need to create a Pipeline. For this case, our model for the Pipeline is `Logistic Regression(max_iter=1000)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d208794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Setup required preprocessing method(s) and model (1 mark)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ct = ColumnTransformer([(\"scaling\",StandardScaler(),[\"area\", \"perimeter\", \"compactness\", \"length of kernel\",\"width of kernel\",\"asymmetry coefficient\",\"length of kernel groove\"])])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "79916456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Setup pipeline (1 mark)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "pipe = Pipeline(steps=[('preprocessor', ct),\n",
    "                      ('classifier', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a64649",
   "metadata": {},
   "source": [
    "### Step 4: Validation (2 marks)\n",
    "\n",
    "To validate the results, we need to use cross-validation. To make sure we are using the best hyperparmeters, we can use a grid search. The parameter grid has been provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63702f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [{'classifier': [LogisticRegression(max_iter=1000)],\n",
    "              'classifier__C': [0.01, 0.1, 1.0, 10.0, 100],\n",
    "              'classifier__fit_intercept': [True, False]}]\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, return_train_score=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6b8c133e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:\n",
      "{'classifier': LogisticRegression(max_iter=1000), 'classifier__C': 10.0, 'classifier__fit_intercept': True}\n",
      "\n",
      "Best cross-validation train score: 0.97\n",
      "Best cross-validation score: 0.95\n",
      "Test-set score: 1.00\n"
     ]
    }
   ],
   "source": [
    "#TO DO: Implement the grid search and print the best parameters, training score, cross-validation score and test score (2 marks)\n",
    "grid.fit(Xtrain,ytrain)\n",
    "\n",
    "print(\"Best params:\\n{}\\n\".format(grid.best_params_))\n",
    "print(\"Best cross-validation train score: {:.2f}\".format(grid.cv_results_['mean_train_score'][grid.best_index_]))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Test-set score: {:.2f}\".format(grid.score(Xtest, ytest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a298fad",
   "metadata": {},
   "source": [
    "### Step 5: Visualize using PCA (5 marks)\n",
    "\n",
    "Now you can visualize the results from Step 4 using PCA. Use the best parameters from the previous step to predict the label for the testing data. For the plot, each data point must be colored based on the class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5a4a7fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Predict target vector labels using best estimator (1 mark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "91b40f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Reduce dimensions of test data for plotting (2 marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fec7570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Plot test points colored by predicted label (2 marks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b865f80b",
   "metadata": {},
   "source": [
    "### Questions (4 marks)\n",
    "\n",
    "1. Did the logistic regression model work well for this dataset? Why would you make this conclusion? \n",
    "1. Did PCA work well for this model? How does this relate to the results from the model used? \n",
    "\n",
    "*ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f81b972",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed521fc",
   "metadata": {},
   "source": [
    "## Part 2: Neural Networks (28 marks)\n",
    "\n",
    "For this assignment, you will be practicing using scikit-learn and TensorFlow to implement basic neural networks (MLP). The dataset we will be using is the energy dataset from Yellowbrick (https://www.scikit-yb.org/en/latest/api/datasets/energy.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c16745",
   "metadata": {},
   "source": [
    "### Step 1: Load data (1 mark)\n",
    "\n",
    "You will need to load the file and split it into the feature matrix and target vector. Note that this dataset has two targets, heating load and cooling load. To retrieve the single target dataset, you need `return_dataset=False`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f39d53a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Load dataset into feature matrix and target vector (1 mark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f706755",
   "metadata": {},
   "source": [
    "### Step 2: Process your dataset (6 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f546a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Check if there are any missing values - if yes, decide how to fill them (1 mark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "55a1b3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Check data type for each column and the number of unique values - do you need to encode any of them? (1 mark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "30488872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Check the range of each feature - do you need to scale your data? (1 mark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "505da56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Split your data into training and testing sets using 20% for test set (1 mark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "84756a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Implement scaling and/or encoding here if needed (2 marks for preprocessing properly or justifying why it isn't needed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606f158c",
   "metadata": {},
   "source": [
    "### Step 3: Implement Neural Network (MLP) (8 marks)\n",
    "\n",
    "### Part 1: Use scikit-learn (3 marks)\n",
    "\n",
    "For each case, you will need to find the predicted target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0c566105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Test using default parameters and max_iter = 10000 (1 mark)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d5349975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Test using two hidden layers with 100 nodes each (1 mark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ff8ed7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Test using three hidden layers with 100 nodes each (1 mark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f3582c",
   "metadata": {},
   "source": [
    "### Part 2: Use TensorFlow (5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "75c17195",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8ef158",
   "metadata": {},
   "source": [
    "Instead of scaling the data using a scikit-learn scaler, you can scale the data using a normalization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad79a75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Define normalization layer (1 mark)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddae1a41",
   "metadata": {},
   "source": [
    "Using `keras.Sequential`, implement an MLP with the same hidden layer setups as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e5197d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Implement MLP with one hidden layer with 100 nodes and the relu activation function (2 marks)\n",
    "# Compile the model with loss='mean_absolute_error' and optimizer=tf.keras.optimizers.Adam(0.001)\n",
    "# Fit the model using validation_split=0.2, verbose=0 and epochs=100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03dea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Repeat with two hidden layers with 100 nodes each and the relu activation function (1 mark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bd5a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Repeat with three hidden layers with 100 nodes each and the relu activation function (1 mark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cbabc6",
   "metadata": {},
   "source": [
    "### Step 4: Compare the accuracy of both methods (4 marks)\n",
    "\n",
    "For this part, calculate the mean absolute error for each model and print in a table using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5c1b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Calculate the MAE for the three scikit-learn tests (1.5 marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4f3926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Calculate the MAE for the three TensorFlow tests (1.5 marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bf09ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Print the results (1 mark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebae05b6",
   "metadata": {},
   "source": [
    "### Questions (5 marks)\n",
    "\n",
    "1. Which model produced the least amount of error?\n",
    "1. Do the results change if you run your code multiple times? Why or why not?\n",
    "1.  Why are the numbers different between the scikit-learn and TensorFlow methods when we used the same number of hidden layers and hidden units per layer? List two potential reasons.\n",
    "\n",
    "*ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc847c4",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353717fd",
   "metadata": {},
   "source": [
    "## Part 3: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae29168",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
